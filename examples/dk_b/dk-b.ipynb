{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 13,
            "id": "ffeb4eee",
            "metadata": {},
            "outputs": [],
            "source": [
                "import logging\n",
                "import sys\n",
                "\n",
                "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
                "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "id": "f1a9eb90-335c-4214-8bb6-fd1edbe3ccbd",
            "metadata": {},
            "outputs": [],
            "source": [
                "# My OpenAI Key\n",
                "import os\n",
                "# read key from file\n",
                "with open('..\\..\\.env', 'r') as fp:\n",
                "    os.environ['OPENAI_API_KEY'] = fp.read().strip()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "id": "8d0b2364-4806-4656-81e7-3f6e4b910b5b",
            "metadata": {},
            "outputs": [],
            "source": [
                "from gpt_index import GPTTreeIndex, SimpleDirectoryReader\n",
                "from IPython.display import Markdown, display"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 16,
            "id": "1298bbb4-c99e-431e-93ef-eb32c0a2fc2a",
            "metadata": {
                "tags": []
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "INFO:root:> Building index from nodes: 42 chunks\n",
                        "> Building index from nodes: 42 chunks\n",
                        "> Building index from nodes: 42 chunks\n",
                        "INFO:root:> Building index from nodes: 4 chunks\n",
                        "> Building index from nodes: 4 chunks\n",
                        "> Building index from nodes: 4 chunks\n",
                        "INFO:root:> [build_index_from_documents] Total LLM token usage: 156996 tokens\n",
                        "> [build_index_from_documents] Total LLM token usage: 156996 tokens\n",
                        "> [build_index_from_documents] Total LLM token usage: 156996 tokens\n",
                        "INFO:root:> [build_index_from_documents] Total embedding token usage: 0 tokens\n",
                        "> [build_index_from_documents] Total embedding token usage: 0 tokens\n",
                        "> [build_index_from_documents] Total embedding token usage: 0 tokens\n"
                    ]
                }
            ],
            "source": [
                "documents = SimpleDirectoryReader('data').load_data()\n",
                "index = GPTTreeIndex(documents)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 17,
            "id": "0b4fe9b6-5762-4e86-b51e-aac45d3ecdb1",
            "metadata": {},
            "outputs": [],
            "source": [
                "index.save_to_disk('index_dk_b.json')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 18,
            "id": "5eec265d-211b-4f26-b05b-5b4e7072bc6e",
            "metadata": {},
            "outputs": [],
            "source": [
                "# try loading\n",
                "new_index = GPTTreeIndex.load_from_disk('index_dk_b.json')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 19,
            "id": "68c9ebfe-b1b6-4f4e-9278-174346de8c90",
            "metadata": {
                "tags": []
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "INFO:root:> Starting query: what is the myth of automation?\n",
                        "> Starting query: what is the myth of automation?\n",
                        "> Starting query: what is the myth of automation?\n",
                        "INFO:root:>[Level 0] Selected node: [5]/[5]\n",
                        ">[Level 0] Selected node: [5]/[5]\n",
                        ">[Level 0] Selected node: [5]/[5]\n",
                        "INFO:root:>[Level 1] Selected node: [2]/[2]\n",
                        ">[Level 1] Selected node: [2]/[2]\n",
                        ">[Level 1] Selected node: [2]/[2]\n",
                        "INFO:root:>[Level 2] Selected node: [1]/[1]\n",
                        ">[Level 2] Selected node: [1]/[1]\n",
                        ">[Level 2] Selected node: [1]/[1]\n",
                        "INFO:root:> [query] Total LLM token usage: 6090 tokens\n",
                        "> [query] Total LLM token usage: 6090 tokens\n",
                        "> [query] Total LLM token usage: 6090 tokens\n",
                        "INFO:root:> [query] Total embedding token usage: 0 tokens\n",
                        "> [query] Total embedding token usage: 0 tokens\n",
                        "> [query] Total embedding token usage: 0 tokens\n"
                    ]
                }
            ],
            "source": [
                "# set Logging to DEBUG for more detailed outputs\n",
                "\n",
                "response = new_index.query(\"what is the myth of automation?\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 20,
            "id": "91581e60-6051-40ae-bba6-8fa08ffbb728",
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/markdown": [
                            "<b>The myth of automation is that it can be used to gain ultimate control over the world, allowing humans to shape their own myths and create new worlds. It is seen as a way to break the rules and limits of the 'mother earth' and to explore the extreme views of technology as a military apparatus.</b>"
                        ],
                        "text/plain": [
                            "<IPython.core.display.Markdown object>"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                }
            ],
            "source": [
                "display(Markdown(f\"<b>{response}</b>\"))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 21,
            "id": "ca10a9c1-9dff-476d-b218-3208a1b8e7f6",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "INFO:root:> Starting query: What did rule did prototyping play in greek myth?\n",
                        "> Starting query: What did rule did prototyping play in greek myth?\n",
                        "> Starting query: What did rule did prototyping play in greek myth?\n",
                        "INFO:root:>[Level 0] Selected node: [1]/[1]\n",
                        ">[Level 0] Selected node: [1]/[1]\n",
                        ">[Level 0] Selected node: [1]/[1]\n",
                        "INFO:root:>[Level 1] Selected node: [5]/[5]\n",
                        ">[Level 1] Selected node: [5]/[5]\n",
                        ">[Level 1] Selected node: [5]/[5]\n",
                        "INFO:root:>[Level 2] Selected node: [5]/[5]\n",
                        ">[Level 2] Selected node: [5]/[5]\n",
                        ">[Level 2] Selected node: [5]/[5]\n",
                        "INFO:root:> [query] Total LLM token usage: 7834 tokens\n",
                        "> [query] Total LLM token usage: 7834 tokens\n",
                        "> [query] Total LLM token usage: 7834 tokens\n",
                        "INFO:root:> [query] Total embedding token usage: 0 tokens\n",
                        "> [query] Total embedding token usage: 0 tokens\n",
                        "> [query] Total embedding token usage: 0 tokens\n"
                    ]
                }
            ],
            "source": [
                "# GPT is confused by the text evidence\n",
                "response = new_index.query(\"What did rule did prototyping play in greek myth?\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 22,
            "id": "4fc3f18a-0ef9-453c-acf8-7aedd784cdcf",
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/markdown": [
                            "<b>Prototyping did not play a role in Greek myth. The context information provided is discussing the insights of Thales, a philosopher from Ancient Greece, and his use of Chronos (scientific facts about the size of the Earth or the progression of time) to exercise his agency as a radical use of the future for governance. There is no mention of prototyping in the context information.</b>"
                        ],
                        "text/plain": [
                            "<IPython.core.display.Markdown object>"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                }
            ],
            "source": [
                "display(Markdown(f\"<b>{response}</b>\"))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 23,
            "id": "ce7c0e3e",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "INFO:root:> Starting query: What is prototype?\n",
                        "> Starting query: What is prototype?\n",
                        "> Starting query: What is prototype?\n",
                        "INFO:root:>[Level 0] Selected node: [2]/[2]\n",
                        ">[Level 0] Selected node: [2]/[2]\n",
                        ">[Level 0] Selected node: [2]/[2]\n",
                        "INFO:root:>[Level 1] Selected node: [3]/[3]\n",
                        ">[Level 1] Selected node: [3]/[3]\n",
                        ">[Level 1] Selected node: [3]/[3]\n",
                        "INFO:root:>[Level 2] Selected node: [2]/[2]\n",
                        ">[Level 2] Selected node: [2]/[2]\n",
                        ">[Level 2] Selected node: [2]/[2]\n",
                        "INFO:root:> [query] Total LLM token usage: 7932 tokens\n",
                        "> [query] Total LLM token usage: 7932 tokens\n",
                        "> [query] Total LLM token usage: 7932 tokens\n",
                        "INFO:root:> [query] Total embedding token usage: 0 tokens\n",
                        "> [query] Total embedding token usage: 0 tokens\n",
                        "> [query] Total embedding token usage: 0 tokens\n"
                    ]
                },
                {
                    "data": {
                        "text/markdown": [
                            "<b>A prototype is a model or example of a product, system, or process that is used to test and develop ideas before creating a final version. It is a way of connecting what exists with what could exist in the future, and is a process driven by actions rather than insights. Prototypes can be used to transform social organizations, culture, and language, and can be used to organize communities beyond the representation of an ideal meaning or regulation.</b>"
                        ],
                        "text/plain": [
                            "<IPython.core.display.Markdown object>"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                }
            ],
            "source": [
                "# GPT is confused by the text evidence\n",
                "response = new_index.query(\"What is prototype?\")\n",
                "display(Markdown(f\"<b>{response}</b>\"))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 24,
            "id": "0f258776",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "INFO:root:> Starting query: How to combine philosophy and design?\n",
                        "> Starting query: How to combine philosophy and design?\n",
                        "> Starting query: How to combine philosophy and design?\n",
                        "INFO:root:>[Level 0] Selected node: [5]/[5]\n",
                        ">[Level 0] Selected node: [5]/[5]\n",
                        ">[Level 0] Selected node: [5]/[5]\n",
                        "INFO:root:>[Level 1] Selected node: [3]/[3]\n",
                        ">[Level 1] Selected node: [3]/[3]\n",
                        ">[Level 1] Selected node: [3]/[3]\n",
                        "INFO:root:>[Level 2] Selected node: [3]/[3]\n",
                        ">[Level 2] Selected node: [3]/[3]\n",
                        ">[Level 2] Selected node: [3]/[3]\n",
                        "INFO:root:> [query] Total LLM token usage: 3927 tokens\n",
                        "> [query] Total LLM token usage: 3927 tokens\n",
                        "> [query] Total LLM token usage: 3927 tokens\n",
                        "INFO:root:> [query] Total embedding token usage: 0 tokens\n",
                        "> [query] Total embedding token usage: 0 tokens\n",
                        "> [query] Total embedding token usage: 0 tokens\n"
                    ]
                },
                {
                    "data": {
                        "text/markdown": [
                            "<b>Combining philosophy and design can be done by exploring the connections between social, cultural, and technological practices. This can be done by looking at forgotten practices in our own culture, as well as exploring the unique collaborations between universities, arts organizations, and local communities, between science and indigenous knowledge, design, and craft. Additionally, by engaging in transglobal and interdisciplinary collaborations, and by utilizing open source technologies, one can gain a better understanding of how to combine philosophy and design.</b>"
                        ],
                        "text/plain": [
                            "<IPython.core.display.Markdown object>"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                }
            ],
            "source": [
                "# GPT is confused by the text evidence\n",
                "response = new_index.query(\"How to combine philosophy and design?\")\n",
                "display(Markdown(f\"<b>{response}</b>\"))"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "ing",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.9.16"
        },
        "vscode": {
            "interpreter": {
                "hash": "718d7dbe35c2992c4ad99e266b72ad7faf17c8703569c5eaa3b1bb48bc338eee"
            }
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
